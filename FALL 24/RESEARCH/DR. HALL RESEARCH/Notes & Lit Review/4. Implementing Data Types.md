## What Data Types are we Working With?
As a complete beginner to HCA myself, the only datatype I've used for simplistic programs are `.jpeg` and `.png` photos. As this project concerns more complex, multilayered astronomical data as well as simulated data, integrating these data types may be more difficult.

### Experimental Data
- We will be using FITS files from telescopes as our experimental data.
- We will be using dump files from PHANTOM SPH simulations as our control/reference data.

FITS files possess a vast range of complex data and, as a result, we must consider the necessary features we'll need to extract:
- **Event Lists and Spectra** contain data regarding energy distributions, spectral lines, and intensity variations.
- **Calibrated Images** express spatial features like disc shape, size, and brightness profiles.
- **3D Data Cubes** possess both spatial and spectral features, but are produced only bt JWST (and not Chandra).

We can use libraries such as `Astropy` and `Astroquery` to read FITS files and extract these features. We can additionally perform image processing to identify and quantify morphological features.

### Simulated Reference Data
- We could potentially use PHANTOM SPH dump files as our reference data.
- We could also post-process this data using MCFOST.
> NOTE: I'm not familiar enough with either to be certain of a workflow to implement, here.

PHANTOM's dump files contain detailed information about simulated particles, including their positions, velocities, densities, and temperatures.
- We could use MCFOST to read dump files and SPLASH to provide simulational data, potentially extracting features analogous to those from the FITS files.
- In doing this, we could calculate quantities such as disc density profiles, temperature distributions, and velocity fields.

## How can we Implement HCA with FITS *and* Simulation Data?
Through **Feature Standardization** and **Integration, we could:
- Normalize the extracted features to ensure they're on a comparable scale, involving techniques like min-max scaling or standard scaling.
- Combine the standardized features from both observed and simulated data into a single dataset suitable for clustering.

We could use a method like **Agglomerative Clustering**:
- Agglomerative Clustering is a bottom-up approach which iteratively groups data points that start as their own clusters into cluster pairs, merging them until all points are grouped.
- Agglomerative Clustering does not require a specified number of clusters in advance, instead building a hierarchy of clusters which we can then explore to determine the optimal number of clusters based on our data: this may be useful given that we don't know all potential inital conditions to expect from accretion discs.
- Using a linkage criterion such as **Ward's Linkage,** we can opt to minimize the variance within each cluster instead of determining associations based on the shortest distance.

## Example Code
I'm not sure if this is the most efficient or optimal approach to this process--this is more an estimated guess using my previous example HCA program as reference.
```
import numpy as np
import os
from astropy.io import fits
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram
import matplotlib.pyplot as plt
```
> - `numpy` is useful for converting image features into arrays.
> - `os` is useful for loading datasets through file and directory operations.
> - `astropy.io` is capable of handling `fits` files.
> - `StandardScaler` is a preprocessing tool used to standardize features by removing the mean and scaling to unit variance.
> - `AgglomerativeClustering` is a bottom-up clustering approach.
> - `dendrogram` and `plt` are useful for visualizing our data.

```
# Example Function to Extract Features from a FITS File
def extract_fits_data(fits_file_path):
    with fits.open(fits_file_path) as hdul:
        sci_data = hdul['SCI'].data
        err_data = hdul['ERR'].data
        dq_data = hdul['DQ'].data

        # Example Feature Extraction (Mean of Science Data)
        mean_sci = np.mean(sci_data)
        features = [mean_sci]
        return features
```
> Here, we're defining an example function that can be used and modified to extract features from a FITS file.
> - Using `astropy.io`'s `fits` function, we are opening our FITS files to access the Header Data Units (HDUs), which are units in FITS files that consist of a header and data.
>   - The header contains metadata in the form of keyword-value pairs, describing data such as dimensions and type, or other relevant information.
>   - The data can be an image, a table, or any other type of array.
> - `hdul` specifically refers to the HDU list, which are a collection of HDUs contained within a FITS file.

```
# Example Function to Extract Features from a PHANTOM Dump File
def extract_dump_data(dump_file_path)
      # Implement reading of PHANTOM dump files & feature extraction (e.g. mean density and temperature)

      # Example Feature Extraction (Mean of Density Data)
      mean_dens = np.mean(dens_data) # Variable should be defined above
      features = [mean_dens]
      return features
```
> This function should extract key data from our PHANTOM dump files (or whatever other data type we may be using).
> - I'm not really sure how to do this (or if this is the optimal way to source data from our simulations), so I don't know what specifically to do yet.

```
# Loading & Standardizing Features from FITS Files
fits_directory = 'path_to_fits_files_directory'

fits_files = [os.path.join (fits_directory), f) for f in os.listdir(fits_directory) if f.endswith('.fits')]

fits_features = [extract_fits_data(file) for file in fits_files]

fits_features_array = np.array(fites_features)

scaler = StandardScaler()
fits_features_scaled = scaler.fit_transform(fits_features_array)
```
> This subsection is responsible for loading and extracting data from FITS files.
> - First, we ensure all FITS files are in a directory (`path_to_fits_files_directory`).
> - Next, we list all FITS files in the directory (files that end with `.fits`), creating a list of full file paths.
> - `fits_features` then iterates through each FITS file path, extracting features using the function we defined earlier (`extract_fits_data`) and storing the results in a list.
> - `fits_features_array` converts the list of features to a `numpy` array and uses `StandardScaler` to standardize the features, ensuring they have a mean of 0 and a standard deviation of 1.

```
# Loading & Standardizing Features from PHANTOM Dump Files
phantom_directory = 'path_to_dump_files_directory'

phantom_files = [os.path.join (phantom_directory), f) for f in os.listdir(phantom_directory) if f.endswith('.dmp')] # !PLACEHOLDER

phantom_features = [extract_phantom_data(file) for file in phantom_files]

phantom_features_array = np.array(phantom_features)

scaler = StandardScaler()
phantom_features_scaled = scaler.fit_transform(phantom_features_array)
```
> This subsection uses the same methodology as the previous one.
> - Because I'm still unsure of what datatype to use for our reference data I've directly copied the same approach for `.fits` files; in practice, this may not be the right approach.
```
# Combining & Standardizing All Features
all_features = np.vstack(fits_features_scaled, phantom_features_scaled)
all_features_scaled = scaler.fit_transform(all_features) # Could be redundant.
```
> This subsection combines and scales all features established in the previously defined lists.
> - `all_features` initializes a `numpy` function which vertically stacks arrays in order to combine the `fits_features_scaled` and `phantom_features_scaled` arrays we defined previously.
> - `all_features_scaled` standardizes the previous array we defined.
```
# Performing Hierarchical Clustering
clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0)
clustering.fit(all_features_scaled)
```
> This snippet performs hierarchical clustering using an Agglomerative Clustering method.

As I wait to receive more information regarding working with PHANTOM simulations and dump files, I'll work more on applying HCA purely to data derived as FITS files from the telescopes.
